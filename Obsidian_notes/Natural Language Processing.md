The normal workflow when dealing with input data in the format of text is:

```info
Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)
```

where the part **turn into numbers** involve the new and additional step in comparison with numerical data.

<mark class='yellow'>Remember</mark>  ðŸ“£: Machine Learning algorithms only accept numerical data

The proccess of turning text into numbers receive the name of #tokenization and #vectorization or #embedding

**Tokenization** is when you conver each word or character into a number
**Vectorization** is when, once you have these numbers, you expand the dimension of each number into a vector which reflects the relationship between the words or characters and their surrounding characters or words.

When dealing with this, there are two main approaches:
1. You can create your own token and vectorization by using sckit functions
2. You can use transfer learning and use an already existing net

## Tokenization ( #tokenization )

One way of doing this is by using the preprocessing layer [`tf.keras.layers.experimental.preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization). This takes the following parameters
-   `max_tokens`Â - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.
-   `standardize`Â - Method for standardizing text. Default isÂ `"lower_and_strip_punctuation"`Â which lowers text and removes all punctuation marks.
-   `split`Â - How to split text, default isÂ `"whitespace"`Â which splits on spaces.
-   `ngrams`Â - How many words to contain per token split, for example,Â `ngrams=2`Â splits tokens into continuous sequences of 2.
-   `output_mode`Â - How to output tokens, can beÂ `"int"`Â (integer mapping),Â `"binary"`Â (one-hot encoding),Â `"count"`Â orÂ `"tf-idf"`. See documentation for more.
-   `output_sequence_length`Â - Length of tokenized sequence to output. For example, ifÂ `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.
-   `pad_to_max_tokens`Â - Defaults toÂ `False`, ifÂ `True`, the output feature axis will be padded toÂ `max_tokens`Â even if the number of unique tokens in the vocabulary is less thanÂ `max_tokens`. Only valid in certain modes, see docs for more

## Embeddings ( #embedding )

As we can use tensorflow to create our tokenz we can use it to embed our tokenz. In this case the preprocessing layer is [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) , an interesting thing to point out is that this layer is trained and the vectors will change as the whole NN is trained. The main parameters are:
-   `input_dim`Â - The size of the vocabulary (e.g.Â `len(text_vectorizer.get_vocabulary()`).
-   `output_dim`Â - The size of the output embedding vector, for example, a value ofÂ `100`Â outputs a feature vector of size 100 for each word.
-   `embeddings_initializer`Â - How to initialize the embeddings matrix, default isÂ `"uniform"`Â which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.
-   `input_length`Â - Length of sequences being passed to embedding layer.
An example use of this layer is:
```jupyter
from tensorflow.keras import layers

embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape
                             output_dim=128, # set size of embedding vector
                             embeddings_initializer="uniform", # default, intialize randomly
                             input_length=max_length, # how long is each input
                             name="embedding_1") 

embedding
```


<mark class='yellow'> Observation </mark> ðŸ“£: When dealing with multi class classification you can use **one hot encodding** ( #onehotencodding) or **label encoded** ( #labelencoded). It is important to note that TensorFlow's CategoricalCrossentropy loss function likes to have one hot encoded labels

